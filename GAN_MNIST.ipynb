{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN_MNIST.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepakupman/GAN_MNIST/blob/master/GAN_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBI2emfXzq-f",
        "colab_type": "text"
      },
      "source": [
        "# MNIST GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZzMCrWGsj79",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzsxUW1YtSub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# samples per batch to load\n",
        "batch_size = 64\n",
        "\n",
        "# convert data to torch.FloatTensor\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# get the training datasets\n",
        "train_data = datasets.MNIST(root=\"data\", train=True, download=True, transform=transform)\n",
        "\n",
        "# prepare data loader\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuOoii1wzv2r",
        "colab_type": "text"
      },
      "source": [
        "# Visualize the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZzqZP7RuO3z",
        "colab_type": "code",
        "outputId": "3981ace3-c84e-46cb-dc7c-5c9531c664c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "# obtain one batch of training images\n",
        "images, _ = next(iter(train_loader))\n",
        "images = images.numpy()\n",
        "\n",
        "# get one image from the batch\n",
        "img = np.squeeze(images[0])\n",
        "\n",
        "# display image\n",
        "fig = plt.figure(figsize=(3, 3))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.imshow(img, cmap='gray')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f4055cf92b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMgAAADFCAYAAAARxr1AAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 216x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ev8AmIDV2lvR",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Define the Model\n",
        "\n",
        "A GAN is comprised of two adversarial networks, a discriminator and a generator.\n",
        "\n",
        "**Discriminator:** The discriminator network is going to be a pretty typical linear classifier. To make this network a universal function approximator, we'll need at least one hidden layer, and these hidden layers should have one key attribute:\n",
        "\n",
        "**Generator:** The generator network will be almost exactly the same as the discriminator network, except that we're applying a tanh activation function to our output layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f-A7CcJ7JyF",
        "colab_type": "text"
      },
      "source": [
        "## Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27_JRAS9289p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pl5L5fGR1eWT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, input_size, hidden_dim, output_size):\n",
        "    super(Discriminator, self).__init__()\n",
        "    # hidden Linear Layers\n",
        "    self.fc1 = nn.Linear(input_size, hidden_dim*4)\n",
        "    self.fc2 = nn.Linear(hidden_dim*4, hidden_dim*2)\n",
        "    self.fc3 = nn.Linear(hidden_dim*2, hidden_dim)\n",
        "    self.fc4 = nn.Linear(hidden_dim, output_size)\n",
        "    \n",
        "    # Dropout layer for regularization\n",
        "    self.dropout = nn.Dropout(0.3)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    # flatten the input\n",
        "    x = x.view(-1, 28*28)\n",
        "    \n",
        "    x = F.leaky_relu(self.fc1(x), negative_slope=0.2)\n",
        "    x = dropout(x)\n",
        "    x = F.leaky_relu(self.fc2(x), negative_slope=0.2)\n",
        "    x = dropout(x)\n",
        "    x = F.leaky_relu(self.fc3(x), negative_slope=0.2)\n",
        "    x = dropout(x)\n",
        "    \n",
        "    # output \n",
        "    return self.fc4(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7R43TEEN87Kf",
        "colab_type": "text"
      },
      "source": [
        "## Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCfEWFd_8zsv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self, input_size, hidden_dim, output_size):\n",
        "    # hidden linear layers\n",
        "    self.fc1 = nn.Linear(input_size, hidden_dim)\n",
        "    self.fc2 = nn.Linear(hidden_dim, hidden_dim*2)\n",
        "    self.fc3 = nn.Linear(hidden_dim*2, hidden_dim*4)\n",
        "    self.fc4 = nn.Linear(hidden_dim*4, output_size)\n",
        "    \n",
        "    # dropout layer\n",
        "    self.dropout = nn.Dropout(0.3)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = F.leaky_relu(self.fc1(x), negative_slope=0.2)\n",
        "    x = dropout(x)\n",
        "    x = F.leaky_relu(self.fc2(x), negative_slope=0.2)\n",
        "    x = dropout(x)\n",
        "    x = F.leaky_relu(self.fc3(x), negative_slope=0.2)\n",
        "    x = dropout(x)\n",
        "    x = F.tanh(self.fc4(x))\n",
        "    \n",
        "    return x\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jH8oFbpQZgB",
        "colab_type": "text"
      },
      "source": [
        "## Model hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqWG-G0R85ys",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Discriminator hyperparams\n",
        "\n",
        "# Size of input image to discriminator (28*28)\n",
        "input_size = 784\n",
        "# Size of discriminator output (real or fake)\n",
        "d_output_size = 1\n",
        "# Size of last hidden layer in the discriminator\n",
        "d_hidden_size = 32\n",
        "\n",
        "# Generator hyperparams\n",
        "\n",
        "# Size of latent vector to give to generator\n",
        "z_size = 100\n",
        "# Size of discriminator output (generated image)\n",
        "g_output_size = 784\n",
        "# Size of first hidden layer in the generator\n",
        "g_hidden_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ME4qrXl7Qe6B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# instantiate discriminator and generator\n",
        "D = Discriminator(input_size, d_hidden_size, d_output_size)\n",
        "G = Generator(z_size, g_hidden_size, g_output_size)\n",
        "\n",
        "# check that they are as you expect\n",
        "print(D)\n",
        "print()\n",
        "print(G)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlkgjcKGQm0R",
        "colab_type": "text"
      },
      "source": [
        "## Discriminator and Generator Losses\n",
        "\n",
        "### Discriminator Losses\n",
        "\n",
        "> * For the discriminator, the total loss is the sum of the losses for real and fake images, `d_loss = d_real_loss + d_fake_loss`. \n",
        "* Remember that we want the discriminator to output 1 for real images and 0 for fake images, so we need to set up the losses to reflect that.\n",
        "* The losses will by binary cross entropy loss with logits, which we can get with [BCEWithLogitsLoss](https://pytorch.org/docs/stable/nn.html#bcewithlogitsloss). This combines a `sigmoid` activation function **and** and binary cross entropy loss in one function.\n",
        "* For the real images, we want `D(real_images) = 1`.\n",
        "* Discriminator generalize better, if the labels are **reduced a bit from 1.0 to 0.9** parameter `smooth`will smooth the labels\n",
        "* The discriminator loss for the fake data is similar. We want `D(fake_images) = 0`, where the fake images are the _generator output_, `fake_images = G(z)`. \n",
        "\n",
        "### Generator Loss\n",
        "\n",
        "> The generator loss will look similar only with flipped labels. The generator's goal is to get `D(fake_images) = 1`. In this case, the labels are **flipped** to represent that the generator is trying to fool the discriminator into thinking that the images it generates (fakes) are real!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVKBpB_BSq1P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate losses\n",
        "def real_loss(D_out, smooth=False):\n",
        "    batch_size = D_out.size(0)\n",
        "    # label smoothing\n",
        "    if smooth:\n",
        "        # smooth, real labels = 0.9\n",
        "        labels = torch.ones(batch_size)*0.9\n",
        "    else:\n",
        "        labels = torch.ones(batch_size) # real labels = 1\n",
        "        \n",
        "    # numerically stable loss\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    # calculate loss\n",
        "    loss = criterion(D_out.squeeze(), labels)\n",
        "    return loss\n",
        "\n",
        "def fake_loss(D_out):\n",
        "    batch_size = D_out.size(0)\n",
        "    labels = torch.zeros(batch_size) # fake labels = 0\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    # calculate loss\n",
        "    loss = criterion(D_out.squeeze(), labels)\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R55mAe5pTQQJ",
        "colab_type": "text"
      },
      "source": [
        "## Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5OYyajHTQmb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Optimizers\n",
        "lr = 0.002\n",
        "\n",
        "# Create optimizers for the discriminator and generator\n",
        "d_optimizer = optim.Adam(D.parameters(), lr)\n",
        "g_optimizer = optim.Adam(G.parameters(), lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPXlg4-ZTY00",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Training\n",
        "\n",
        "Training will involve alternating between training the discriminator and the generator. We'll use our functions `real_loss` and `fake_loss` to help us calculate the discriminator losses in all of the following cases.\n",
        "\n",
        "### Discriminator training\n",
        "1. Compute the discriminator loss on real, training images        \n",
        "2. Generate fake images\n",
        "3. Compute the discriminator loss on fake, generated images     \n",
        "4. Add up real and fake loss\n",
        "5. Perform backpropagation + an optimization step to update the discriminator's weights\n",
        "\n",
        "### Generator training\n",
        "1. Generate fake images\n",
        "2. Compute the discriminator loss on fake images, using **flipped** labels!\n",
        "3. Perform backpropagation + an optimization step to update the generator's weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvuSEb0RTSYM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training hyperparams\n",
        "num_epochs = 100\n",
        "\n",
        "# keep track of loss and generated, \"fake\" samples\n",
        "samples = []\n",
        "losses = []\n",
        "\n",
        "# Get some fixed data for sampling. These are images that are held\n",
        "# constant throughout training, and allow us to inspect the model's performance\n",
        "sample_size=16\n",
        "fixed_z = np.random.uniform(-1, 1, size=(sample_size, z_size))\n",
        "fixed_z = torch.from_numpy(fixed_z).float()\n",
        "\n",
        "# train the networks\n",
        "D.train()\n",
        "G.train()\n",
        "\n",
        "for epoch in tqdm_notebook(range(num_epochs)):\n",
        "  for r_image, _ in tqdm_notebook(train_loader):\n",
        "    batch_size = r_image.size(0)\n",
        "    \n",
        "    # rescale image into [-1, 1]\n",
        "    r_image = r_image * 2 - 1\n",
        "    \n",
        "    # Train the Discriminator\n",
        "    # train with real image\n",
        "    r_out = D(r_image)\n",
        "    r_loss = real_loss(r_out, smooth=True)\n",
        "    \n",
        "    # train with fake image\n",
        "    z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n",
        "    z = torch.from_numpy(z).float()\n",
        "    fake_image = G(z)\n",
        "    \n",
        "    # calculate discriminator loss for fake image\n",
        "    f_out = D(fake_image)\n",
        "    f_loss = fake_loss(f_out)\n",
        "    \n",
        "    # add both losses\n",
        "    d_loss = r_loss + f_loss\n",
        "    \n",
        "    # backpropogation\n",
        "    d_optimizer.zero_grad()\n",
        "    d_loss.backward()\n",
        "    d_optimizer.step()\n",
        "    \n",
        "    #####################\n",
        "    # Train the Generator\n",
        "    \n",
        "    # Generate latent vector\n",
        "    z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n",
        "    z = torch.from_numpy(z).float()\n",
        "    \n",
        "    # Generate fake image from latent vector\n",
        "    fake_image = G(z)\n",
        "    \n",
        "    # calculate discriminator loss for generated image\n",
        "    d_image = D(fake_image)\n",
        "    g_loss = real_loss(d_image)\n",
        "    \n",
        "    # perform backpropogation\n",
        "    g_optimizer.zero_grad()\n",
        "    g_loss.backward()\n",
        "    g_optimizer.step()\n",
        "    \n",
        "    \n",
        "  # append d_loss and g_loss\n",
        "  losses.append((d_loss.item(), g_loss.item()))\n",
        "  \n",
        "  # generate and save fake image after each epoch\n",
        "  # turn on eval mode for G\n",
        "  g.eval()\n",
        "  samples.append(G(fixed_z))\n",
        "  # turn on train mode for G\n",
        "  G.train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Id6iww0hJ6V",
        "colab_type": "text"
      },
      "source": [
        "## Training loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOMlHK0ThNkN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "losses = np.array(losses)\n",
        "plt.plot(losses[0], label=\"Discriminator\")\n",
        "plt.plot(losses[1], label=\"Generator\")\n",
        "plt.title(\"Training Losses\")\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1CLjRWwip15",
        "colab_type": "text"
      },
      "source": [
        "## Generator samples from training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BklY3LVcitAP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def view_samples(epoch, samples):\n",
        "    fig, axes = plt.subplots(figsize=(7,7), nrows=4, ncols=4, sharey=True, sharex=True)\n",
        "    for ax, img in zip(axes.flatten(), samples[epoch]):\n",
        "        img = img.detach()\n",
        "        ax.xaxis.set_visible(False)\n",
        "        ax.yaxis.set_visible(False)\n",
        "        im = ax.imshow(img.reshape((28,28)), cmap='Greys_r')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7B-R1eXji3lb",
        "colab_type": "text"
      },
      "source": [
        "### View sample image saved after each epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uM_Tkfbixdd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "view_samples(-1, samples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccFUJhGNjbCX",
        "colab_type": "text"
      },
      "source": [
        "### Generate Image from random noise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AR7rwKJqjM09",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# randomly generated, new latent vectors\n",
        "sample_size=16\n",
        "rand_z = np.random.uniform(-1, 1, size=(sample_size, z_size))\n",
        "rand_z = torch.from_numpy(rand_z).float()\n",
        "\n",
        "G.eval() # eval mode\n",
        "# generated samples\n",
        "rand_images = G(rand_z)\n",
        "\n",
        "# 0 indicates the first set of samples in the passed in list\n",
        "# and we only have one batch of samples, here\n",
        "view_samples(0, [rand_images])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}